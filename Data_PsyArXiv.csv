ID,Title,Abstract,Date Published,Tags,DOI,URL,Contributors
2m4da_v1,Judgements of Deepfake Sexual Abuse Victims Differ as a Function of Facial Versus Body Likenesses,"We are witnessing exponential growth in the use of machine learning to create fake – yet indistinguishable - sexual media of others without their consent. Though there is an emerging understanding of the impact that deepfake sexual abuse (DSA) has on its victims and societal understanding thereof, this knowledge pertains entirely to individuals whose facial likeness has been used within said material, with little-to-no attention paid to those whose bodies are used as the canvas; individuals who are predominantly sex workers. Across 321 participants (Mage = 45.70 years, SD = 15.88; 48.9% female), vignettes were used to explore differences in societal judgements of DSA for victims whose face (versus body) was used to generate DSA material, and whether they were labelled as a sex worker. Though perceived criminality did not differ across conditions, participants allocated more blame and less anticipated harm to DSA victims whose body, relative to whose face, was used. This effect was enhanced in vignettes labelling them as sex workers. When exploring correlations using demographics, beliefs, and personality traits, being older, male, and viewing sex work as ‘a choice’ and/or ‘deviant’ predicted greater victim blame, lower perceived criminality, and less anticipated harm. High self-reported empathy was the only predictor of greater anticipated harm. Results indicate the importance of understanding broader impacts of DSA - regardless of the victim - for stakeholders within the criminal justice system, and a continued need to generate public awareness of DSA through international policy.",2025-03-12T11:24:07.056983,"deepfake sexual abuse,image-based sexual abuse,OnlyFans,sex workers,technology-facilitated sexual violence",,https://osf.io/preprints/psyarxiv/2m4da_v1/,"Dean Fido, Harriet Goldfinch, Dominic Ruddy, Craig Harper"
3yx2r_v1,Perceptual judgements are resistant to the advisor's perceived level of trustworthiness: a deep fake approach,"As we navigate our environment, we often make spontaneous judgments about others. Trustworthiness is a key characteristic that we judge instantly and then use when making decisions, especially when we are uncertain. While the effect of trustworthiness on social behaviour has been extensively investigated, it remains unclear how trustworthiness affects more ‘basic’ cognitive processes such as perceptual decision-making. The present study aims to fill this gap. In the first experiment (N = 100), we validated a new trustworthiness manipulation by using deep fake technology to create animated versions of perceptually trustworthy, untrustworthy, and neutral static computer-generated faces. In the second experiment (N = 199), the deep fake procedure was applied to a new set of trustworthy and untrustworthy faces that served as advisors during a perceptual decision-making task. Here participants had to indicate the direction of dots that were either moving coherently to the left or to the right (i.e., random dot motion task). Contrary to our predictions, participants did not follow the advice of the trustworthy advisors more often than the advice of the untrustworthy advisors. We did find that participants decided faster and were more confident when following the advice, but this was also not influenced by the trustworthiness of the advisors. We integrate our findings within theoretical frameworks of advice taking, domain specificity of facial trustworthiness, and task requirements.",2024-05-09T12:35:09.876570,"advice taking,deep fake,faces,perceptual decision making,social cognition,Trustworthiness",,https://osf.io/preprints/psyarxiv/3yx2r_v1/,"Mathias Van der Biest, Sam Verschooren, Frederick Verbruggen, Marcel Brass"
4ms5a_v1,Deepfaked online content is highly effective in manipulating people’s attitudes and intentions,"In recent times, disinformation has spread rapidly through social media and news sites, biasing our (moral) judgements of other people and groups. “Deepfakes”, a new type of AI-generated media, represent a powerful new tool for spreading disinformation online. Although Deepfaked images, videos, and audio may appear genuine, they are actually hyper-realistic fabrications that enable one to digitally control what another person says or does. Given the recent emergence of this technology, we set out to examine the psychological impact of Deepfaked online content on viewers. Across seven preregistered studies (N = 2558) we exposed participants to either genuine or Deepfaked content, and then measured its impact on their explicit (self-reported) and implicit (unintentional) attitudes as well as behavioral intentions. Results indicated that Deepfaked videos and audio have a strong psychological impact on the viewer, and are just as effective in biasing their attitudes and intentions as genuine content. Many people are unaware that Deepfaking is possible; find it difficult to detect when they are being exposed to it; and most importantly, neither awareness nor detection serves to protect people from its influence. All preregistrations, data and code available at osf.io/f6ajb.",2021-03-08T21:28:34.047805,"Attitudes,Deepfake,Disinformation,Implicit,Intentions,Psychology",,https://osf.io/preprints/psyarxiv/4ms5a_v1/,"Sean Hughes, Ohad Fried, Melissa Ferguson, Ciaran Hughes, Rian Hughes, Xinwei Yao, Ian Hussey"
5bmav_v2,Understanding how survivors of non-consensual intimate image dissemination are perceived by UK and Norwegian respondents,"Purpose: In popular media, ‘revenge pornography’ refers to the non-consensual sharing of intimate images (NCSII) of another. Despite survivors of NCSII facing long-term consequences, they still face victim-blaming attitudes. Extant literature has typically sampled from countries where NCSII has long been illegal, such as the United Kingdom (UK); neglecting perspectives from countries lacking NCSII-specific legislation, such as Norway at the time of data collection. 
Methods: Participants (n = 477) from the UK and Norway responded to vignettes depicting NCSII, which differed by the survivor-perpetrator relationship depicted (i.e., casual vs. committed). 
Results: Controlling for participant sex and psychopathic personality traits (previously implicated in judgements of image-based sexual abuse), UK citizens perceived NCSII to have worse impacts on survivors than Norwegian citizens. Moreover, data trends suggested participants attributed increased victim-blame in vignettes featuring casual relationships, with higher self-reported psychopathic personality traits predicting judgements associated with viewing NCSII as less criminal in nature. 
Conclusion: These findings emphasise a need to better understand the role of legislation in public perceptions of NCSII (and image-based sexual abuse more broadly) and the need to be conscious about further exploring technology-facilitated crime internationally.",2025-03-27T10:15:31.682898,"image-based sexual abuse,non-consensual sharing of intimate images,psychopathy,revenge pornography,victim blame",,https://osf.io/preprints/psyarxiv/5bmav_v2/,"Dean Fido, Anny C. Hesbøl, Anthony Danby"
6qr7t_v1,Development and validation of the Beliefs about Revenge Pornography Questionnaire,"Revenge pornography has become an increasingly prominent topic in social and legislative discussions about sexual crime, but has received relatively little attention within psychological research. Here, we leveraged existing theorizing in the area of sexual offending proclivity to systematically develop and validate of a measure of beliefs about revenge pornography. Using a large international community sample (N = 511) we found our ‘Beliefs about Revenge Pornography Questionnaire (BRPQ)’ to be comprised of three underpinning domains: ‘Victims as Responsible’, ‘Sociological Explanations’, and ‘Revenge Pornography as a Sexual Offense’. Concurrent validity is demonstrated through relationships with trait empathy, belief in a just world, dark personality traits, and rape myth acceptance. Randomly dividing the sample, we also show that the BRPQ predicts both revenge pornography proclivity (n = 227) and social judgements of this type of offending (n = 233). Implications and future directions are discussed.",2020-11-28T22:38:34.394099,"forensic psychology,images based sexual abuse,offense proclivity,revenge pornography,scale development,scale validation,sexual abuse,social attitudes",,https://osf.io/preprints/psyarxiv/6qr7t_v1/,"Craig A. Harper, Lorraine Smith, Jessie Leach, Neil Daruwala, Dean Fido"
7bm45_v1,Examining Perceptions of Sextortion in the General Population,"The non-consensual sharing of sexual images of another (or threats thereof) represents an increasingly important and pervasive issue in forensic psychology. However, despite recent legislative developments within the United Kingdom, there still exists a need for greater understanding of perceptions within the general public of the dangers and manifestations of image-based sexual abuse and the process by which people are being (s)extorted using their private images. Ten participants (Mage = 24.8 years; SD = 9.33; 70% female) took part in semi-structured interviews which were analysed using an inductive thematic analytic approach, wherein the themes of [1] Education as a Prevention against Image Based Sexual Abuse, [2] Victim Blaming Prevalence in Society, [3] Stereotypes about Victim and Perpetrator, and [4] Low Priority for Criminal Justice were explored. Taken together, data suggests that although there exists a ‘working knowledge’ of image-based sexual abuse, the concept of sextortion is understood to a lesser extent with clear implications for the need for educational policies to better inform younger members of society of associated dangers and legalities.",2021-10-11T12:46:44.301696,"Image-Based Sexual Abuse,Revenge Pornography,Sextortion,Thematic Analysis",,https://osf.io/preprints/psyarxiv/7bm45_v1/,"Anisha Gohil, Dean Fido"
a9dwe_v1,Cognitive flexibility but not cognitive styles influence deepfake detection skills and metacognitive accuracy,"Background: Deepfakes are AI-generated synthetic media that is increasingly used by cybercriminals to impersonate other individuals during remote social engineering attacks. Previous studies indicated that political orientation is associated with deepfake detection abilities, while being an IT professional is not. Little is known about the cognitive factors predicting individual differences in deepfake detection abilities. In this study, we assess the role of cognitive styles and cognitive flexibility on deepfake recognition skills and metacognitive accuracy. Methods: Cognitive styles and flexibility were measured using an embedded figures test that included a hidden cognitive flexibility task. 247 participants were tasked with rating a series of short video clips as either deepfake or authentic. Metacognitive accuracy was measured as prospective judgements of deepfake detection abilities controlling for actual performance. Results: Cognitive styles were not associated with deepfake detection performance. Cognitively flexible individuals were better at detecting deepfakes and had higher metacognitive accuracy than individuals who were less cognitively flexible. Conclusion: This is the first study assessing the role of cognitive styles and cognitive flexibility in deepfake detection skills and metacognitive judgements about deepfake detection abilities. Our results indicate that cognitively flexible individuals are better at detecting deepfakes and self-assessing social engineering susceptibility.",2023-02-14T15:03:01.665519,"cognitive flexibility,cognitive security,cognitive styles,cybersecurity,deepfake detection,field dependence,field independence,neuroergonomics,perception,social engineering,synthetic media",,https://osf.io/preprints/psyarxiv/a9dwe_v1/,"Torvald F. Ask, Ricardo Lugo, Jonas fritsch, Karl Veng, Jonathan Eck, Muhammed-Talha Özmen, Basil Bärreiter, Benjamin J. Knox, Stefan Sütterlin"
b4fgc_v1,Understanding Image-Based Sexual Abuse through Greek Public and Legal Lenses: A Constructionist Thematic Analysis,"Image-Based Sexual Abuse (IBSA), including deepfake sexual abuse - where fake yet lifelike sexual content is generated of non-consenting persons - constitutes a growing form of digitally mediated gender-based violence that remains largely under-researched within non-Anglophonic contexts. This study explores how IBSA is constructed and perceived by both laypeople and lawyers in Greece, a Southern European setting characterised by economic precarity, traditional gender norms, and evolving yet challenging legal frameworks. Drawing on semi-structured interviews with 21 participants either originating from or living in Greece (n = 16 lay persons, n = 5 legal professionals), we employed constructionist thematic analysis to examine how participants discursively frame IBSA, its motivations, barriers to reporting, legal challenges, and preventative measures. Five themes were identified: (1) constructions of IBSA as gendered violence motivated by control, humiliation, and financial exploitation; (2) barriers to reporting shaped by shame, stigma, and widespread mistrust in police institutions; (3) legal barriers related to financial inaccessibility and fragmented, outdated legal frameworks; (4) the central role of informal support networks alongside calls for education and public awareness; and (5) deepfake technologies as an emergent form of economic exploitation, particularly impacting sex workers within a legal vacuum. The findings highlight the need for structurally informed, context-sensitive responses to IBSA that address the intersections of gender, law, technology, and economic vulnerability.",2025-07-03T10:04:19.946226,,,https://osf.io/preprints/psyarxiv/b4fgc_v1/,"Anastasia Rousaki, Dean Fido"
hwujb_v1,"Individual Deep Fake Recognition Skills are Affected by Viewers’ Political Orientation, Agreement with Content and Device Used","AI-generated “deep fakes” are becoming increasingly professional and can be expected to become an essential tool for cybercriminals conducting targeted and tailored social engineering attacks, as well as for others aiming for influencing public opinion in a more general sense. While the technological arms race is resulting in increasingly efficient forensic detection tools, these are unlikely to be in place and applied by common users on an everyday basis any time soon, especially if social engineering attacks are camouflaged as unsuspicious conversations. To date, most cybercriminals do not yet have the necessary resources, competencies or the required raw material featuring the target to produce perfect impersonifications. To raise awareness and efficiently train individuals in recognizing the most widespread deep fakes, the understanding of what may cause individual differences in the ability to recognize them can be central. Previous research suggested a close relationship between political attitudes and top-down perceptual and subsequent cognitive processing styles. In this study, we aimed to investigate the impact of political attitudes and agreement with the political message content on the individual’s deep fake recognition skills.
In this study, 163 adults (72 females = 44.2%) judged a series of video clips with politicians’ statements across the political spectrum regarding their authenticity and their agreement with the message that was transported. Half of the presented videos were fabricated via lip-sync technology. In addition to the particular agreement to each statement made, more global political attitudes towards social and economic topics were assessed via the Social and Economic Conservatism Scale (SECS).
Data analysis revealed robust negative associations between participants’ general and in particular social conservatism and their ability to recognize fabricated videos. This effect was pronounced where there was a specific agreement with the message content. Deep fakes watched on mobile phones and tablets were considerably less likely to be recognized as such compared to when watched on stationary computers.
To the best of our knowledge, this study is the first to investigate and establish the association between political attitudes and interindividual differences in deep fake recognition. The study further supports very recently published research suggesting relationships between conservatism and perceived credibility of conspiracy theories and fake news in general. Implications for further research on psychological mechanisms underlying this effect are discussed.",2021-12-30T14:24:35.177365,"conservatism,cybercrime,deep fakes,deepfakes,fake news,perception,political orientation,recognition,social engineering,social media",10.1007/978-3-031-35017-7_18,https://osf.io/preprints/psyarxiv/hwujb_v1/,"Stefan Sütterlin, Torvald F. Ask, Sophia Mägerle, Sandra Glöckler, Leandra Wolf, Julian Schray, Alaya Chandi, Teodora Bursac, Ali Khodabakhsh, Benjamin J. Knox"
knvby_v1,"How deepfake quality, media literacy, and personal attitudes shape detection, liking, and social media sharing of political deepfakes","The increasing realism and accessibility of political deepfakes pose serious risks to democratic discourse by blurring the line between authentic and manipulated media. The present study examined how deepfake quality, media literacy, and attitudes toward the person depicted in the video shape individuals’ ability to detect political deepfakes, their emotional responses (liking), and behavioral intentions (social media sharing). A total of 1,124 participants from the United Kingdom, Slovenia, and Italy viewed manipulated videos about climate change and immigration that varied in quality. The results showed that high-quality deepfakes were less likely to be detected and received more positive evaluations than low-quality ones. In contrast, quality did not significantly influence sharing intention. Media literacy and attitudes toward the person in the video emerged as strong predictors across outcomes; higher media literacy improved detection and reduced liking and sharing, while more positive attitudes decreased detection and increased both liking and sharing. Mediation analyses demonstrated that liking partially mediated the link between detection and sharing intention. Furthermore, moderated mediation models revealed that this indirect effect was stronger among individuals with high media literacy (for climate change videos) and favorable attitudes toward the video’s subject (for both topics). Overall, the study highlights the need for interventions that address not only detection skills but also emotional and motivational susceptibility to persuasive synthetic media. Enhancing media literacy and implementing platform-level friction mechanisms may help curb the spread of political deepfakes online.",2025-05-19T13:56:11.518654,"attitudes,deepfake,detection,liking,media literacy,quality,sharing",,https://osf.io/preprints/psyarxiv/knvby_v1/,"Nejc Plohl, Izidor Mlakar, Letizia Aquilino, Michele Brienza, Piercosma Bisconti, Urška Smrke"
myu8h_v1,The shallow of your smile: The ethics of expressive vocal deepfakes,"Rapid technological advances in artificial intelligence are creating opportunities for real-time algorithmic modulations of a person’s facial and vocal expressions, or “deep-fakes”. These developments raises unprecedented societal and ethical questions which, despite much recent public awareness, are still poorly understood from the point of view of moral psychology. We report here on an empirical study conducted on N=303 online participants, who evaluated the acceptability of vignettes describing potential applications of expressive voice transformation technology. We found that vocal deep-fakes were generally well accepted in the population, notably in a therapeutic context and for emotions judged otherwise difficult to control, and surprisingly, even if the user lies to their interlocutors about using them. Unlike other emerging technologies like autonomous vehicles, there was no evidence of social dilemma in which one would e.g. accept for others what they resent for themselves. The only real obstacle to the massive deployment of vocal deep-fakes appears to be situations where they are applied to a speaker without their knowing, but even the acceptability of such situations was modulated by individual differences in moral values and attitude towards science-fiction.",2021-04-13T12:23:07.530986,,,https://osf.io/preprints/psyarxiv/myu8h_v1/,"Nadia Guerouaou, Guillaume Vaiva, Jean-Julien Aucouturier"
pqwec_v1,Examining the Gendered Impacts of Technology-Facilitated Sexual Violence: A Mixed Methods Approach,"This study employed a mixed methods approach, integrating quantitative online survey data (N = 333; Mage = 33.91; 63% women) with qualitative interview data (N = 10; Agerange = 24-46; 50% women) to gain a more comprehensive understanding of TFSV. We found that victims of TFSV experienced anxiety, stress, depression, loss of control, mistrust, multiple victimizations, poor academic/occupation functioning, problematic alcohol consumption, embarrassment, and online behaviour changes (e.g., limiting personal information online) due to TFSV victimization. Individuals who experienced online image-based abuse reported greater distress on items of depression, anxiety, and occupational/academic functioning than did victims of other types of TFSV. The current study provides partial support for the gender similarities hypothesis that TFSV is not exclusively a gender-based harm; our findings suggest that women and men’s TFSV experiences are similar for most TFSV types. Overall, the present study demonstrates the negative impact TFSV has for both women and men and highlights the need for greater awareness and increased support for all victims of this form of sexual violence.",2021-11-08T18:12:34.754104,"gender,internet abuse,mixed methods,sexual harassment,sexual violence,technology",,https://osf.io/preprints/psyarxiv/pqwec_v1/,"Amanda champion, Flora Blanchette, Devinder Khera, Cory Pedersen"
pwmqu_v1,Intrasexual competition as a predictor of women’s judgements of revenge pornography offending,"Recent legislative developments have led to a marked increase in the empirical investigation of motivations and judgements of so-called acts of ‘revenge pornography’ offending. In two independently-sampled studies, we used moderation analyses to investigate whether higher levels of intrasexual competition predicted more lenient judgements of revenge pornography offences as a function of sex (Study 1, N = 241), and whether such relationships would be further moderated by physical attractiveness (Study 2, N = 402). Potential covariates of callous-unemotional traits, empathy, and victimization history were controlled for. Opposing our hypotheses, we consistently observed a trend for higher levels of intrasexual competition being associated with more lenient judgements of revenge pornography offences involving male victims by female participants. The results are discussed in terms of intrasexual competition potentially sharing variance with unobserved constructs in the wider sexological literature, and of the key relevance of these findings for future empirical investigation into judgements of non-consensual image-based offending. Open data and a preprint of this paper are available at https://osf.io/y29fq/?view_only=568a2c403fcf428280914c149063db95.",2018-12-11T18:02:26.296367,"callous-unemotional traits,empathy,intrasexual competition,non-consensual image-based offending,revenge porn",,https://osf.io/preprints/psyarxiv/pwmqu_v1/,"Dean Fido, Craig A. Harper, Mia Davis, Dominic Petronzi, Sophie Worrall"
t7jfk_v1,Seeing is Believing: The Continued Influence of a Known AI-Generated 'Deepfake' Video,"Advances in artificial intelligence mean it is becoming easier to create highly realistic deepfake videos, which can appear to show someone doing or saying something they did not in fact do or say. Deepfakes may present a threat to individuals and society: for example, deepfakes could be used to influence elections by discrediting political opponents. Psychological research suggests that people cannot reliably identify deepfakes and thus can be influenced by their content. However, little is yet known about the potential impact of a deepfake video which has been explicitly identified and flagged as fake. We explored this issue with two preregistered studies in which participants were shown a bespoke deepfake video of a man appearing to admit to committing a crime. Participants were then asked questions about the man’s guilt, to test how they had been influenced by the video’s content. We found that most participants relied on the content of a deepfake video, even when they had been explicitly warned beforehand that it was fake. This result was observed even with participants who indicated that they believed the warning and knew the video to be fake. We also found this specific warning to be no more effective than a generic warning about the existence of deepfake videos. Our findings suggest that identifying and flagging deepfake videos will not entirely negate their influence. This has implications for legislators, policy makers, and regulators of social media platforms and online news.",2024-05-23T11:27:21.719145,"artificial intelligence,deepfake,misinformation,psychology,social media",,https://osf.io/preprints/psyarxiv/t7jfk_v1/,"Simon Clark, Stephan Lewandowsky"
vpydn_v1,Delineating non-consensual sexual image offending: Towards an empirical approach,"The topic of non-consensual sexual images has become an increasingly important issue within the social policy landscape. Social and legal scholars have advocated for these behaviours to be designated sexual offences due to the mode of perpetration of these behaviours, but are explicit in their rejection of a sexual element being important in the motivations underpinning such behaviours. However, this rejection is inconsistent with the core theoretical models related to sexual offending. In this article, we outline some of the potential psychological concepts that may help us to understand how and why people engage in a range of non-consensual sexual image offences, such as revenge pornography, upskirting, deepfake media production, and cyber-flashing. In doing so, we aim to begin to bridge the gap between legal scholars and psychological scientists, and develop a more comprehensive and theoretically coherent approach to studying this important social topic.",2019-06-12T19:58:21.005041,"courtship disorder,cyber-flashing,deepfake,dick pics,exhibitionism,non-consensual sexual images,pornography,revenge pornography,sexual offending,upskirting",,https://osf.io/preprints/psyarxiv/vpydn_v1/,"Craig A. Harper, Dean Fido, Dominic Petronzi"
whd4b_v1,The shallow of your smile: The ethics of expressive vocal deepfakes,"Rapid technological advances in artificial intelligence are creating opportunities for real-time algorithmic modulations of a person’s facial and vocal expressions, or “deep- fakes”. These developments raises unprecedented societal and ethical questions which, despite much recent public awareness, are still poorly understood from the point of view of moral psychology. We report here on an experimental ethics study conducted on a sample of N=303 participants (predominantly young, western and educated), who evaluated the acceptability of vignettes describing potential applications of expressive voice transformation technology. We found that vocal deep-fakes were generally well accepted in the population, notably in a therapeutic context and for emotions judged otherwise difficult to control, and surprisingly, even if the user lies to their interlocutors about using them. Unlike other emerging technologies like autonomous vehicles, there was no evidence of social dilemma in which one would e.g. accept for others what they resent for themselves. The only real obstacle to the massive deployment of vocal deep-fakes appears to be situations where they are applied to a speaker without their knowing, but even the acceptability of such situations was modulated by individual differences in moral values and attitude towards science-fiction.",2021-07-28T08:41:25.618196,"emotions,experimental ethics,vocal deepfakes,voice transformations",,https://osf.io/preprints/psyarxiv/whd4b_v1/,"Nadia Guerouaou, Guillaume Vaiva, Jean-Julien Aucouturier"
yq8f5_v1,"Celebrity status, sex, and variation in psychopathy predicts judgements of and proclivity to generate and distribute deepfake pornography","With the advent of means to generate and disseminate fake, sexualised images of others for the purposes of financial gain, harassment, or sexual gratification, there is a need to assess and understand the public’s awareness and judgements of said behaviour. In two independently-sampled studies, we used moderation (Study 1) and linear mixed effects (Study 2) analyses to investigate whether judgements of deepfaking differed as a function of victim status (celebrity, non-celebrity), victim and participant demographics, and image use (sharing, own sexual gratification), whilst controlling for the potential covariates of psychopathy and beliefs about a just world. We consistently observed more lenient judgements of deepfake generation and dissemination for victims who were celebrities and male, and when images were created for self-sexual gratification rather than being shared. Moreover, lenient judgements, as well as proclivity to act were predicted by greater levels of psychopathy. We discuss our findings in the context of needing to qualitatively understand the general public’s rationale for said disparity in judgements, as well as identifying and combating barriers to disclose victimisation. Open data and a preprint of this paper are available at https://osf.io/fp85q/?view_only=24ed2820782f4c2f9c19352f97d58611.",2020-11-15T17:11:36.789136,"deepfake media production,judgements,non-consensual image-based offending,psychopathy",,https://osf.io/preprints/psyarxiv/yq8f5_v1/,"Dean Fido, Jaya Rao, Craig A. Harper"
zhmy7_v1,Deepfake Detection in Super-Recognizers and Police Officers,"The present study is the first empirical investigation of the relationshion between human deepfake detection performance (DDP) and individuals' face identity processing ability. Using videos from the Deepfake Detection Challenge, we investigated DDP in two unique observer groups: Super-Recognizers (SRs) and ""normal"" officers from within the 18K members of the Berlin Police. SRs were identified either via previously proposed lab-based procedures or the only existing tool for SR identification involving increasingly challenging, authentic forensic material: the Berlin Test For Super-Recognizer Identification (beSure®). Participants judged either pairs of videos, or single videos in a 2-alternative forced-choice decision setting (i.e., which of the pair, or whether a single video was a deepfake or not). We explored speed-accuracy trade-offs, compared DDP between lab-identified SRs and non-SRs, and police officers as a function of their independently measured face identity processing (FIP) ability. Interestingly, we found no relationship between DDP and FIP ability. Further work using static deepfakes created with current state-of-the-art generative models is needed to determine the value of SR deployment for deepfake detection in law enforcement.",2024-01-13T20:21:39.496175,"deepfake detection,facial identity processing,law enforcement,policing,Super-Recognizers",10.1109/MSEC.2024.3371030,https://osf.io/preprints/psyarxiv/zhmy7_v1/,"Meike Ramon, Matthew J Vowels, Matthew Groh"
