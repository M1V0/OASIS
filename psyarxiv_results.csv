ID,Title,Abstract,Date Published,DOI,URL,Authors
knvby_v1,"How deepfake quality, media literacy, and personal attitudes shape detection, liking, and social media sharing of political deepfakes","The increasing realism and accessibility of political deepfakes pose serious risks to democratic discourse by blurring the line between authentic and manipulated media. The present study examined how deepfake quality, media literacy, and attitudes toward the person depicted in the video shape individuals’ ability to detect political deepfakes, their emotional responses (liking), and behavioral intentions (social media sharing). A total of 1,124 participants from the United Kingdom, Slovenia, and Italy viewed manipulated videos about climate change and immigration that varied in quality. The results showed that high-quality deepfakes were less likely to be detected and received more positive evaluations than low-quality ones. In contrast, quality did not significantly influence sharing intention. Media literacy and attitudes toward the person in the video emerged as strong predictors across outcomes; higher media literacy improved detection and reduced liking and sharing, while more positive attitudes decreased detection and increased both liking and sharing. Mediation analyses demonstrated that liking partially mediated the link between detection and sharing intention. Furthermore, moderated mediation models revealed that this indirect effect was stronger among individuals with high media literacy (for climate change videos) and favorable attitudes toward the video’s subject (for both topics). Overall, the study highlights the need for interventions that address not only detection skills but also emotional and motivational susceptibility to persuasive synthetic media. Enhancing media literacy and implementing platform-level friction mechanisms may help curb the spread of political deepfakes online.",2025-05-19T13:56:11.518654,,,
2m4da_v1,Judgements of Deepfake Sexual Abuse Victims Differ as a Function of Facial Versus Body Likenesses,"We are witnessing exponential growth in the use of machine learning to create fake – yet indistinguishable - sexual media of others without their consent. Though there is an emerging understanding of the impact that deepfake sexual abuse (DSA) has on its victims and societal understanding thereof, this knowledge pertains entirely to individuals whose facial likeness has been used within said material, with little-to-no attention paid to those whose bodies are used as the canvas; individuals who are predominantly sex workers. Across 321 participants (Mage = 45.70 years, SD = 15.88; 48.9% female), vignettes were used to explore differences in societal judgements of DSA for victims whose face (versus body) was used to generate DSA material, and whether they were labelled as a sex worker. Though perceived criminality did not differ across conditions, participants allocated more blame and less anticipated harm to DSA victims whose body, relative to whose face, was used. This effect was enhanced in vignettes labelling them as sex workers. When exploring correlations using demographics, beliefs, and personality traits, being older, male, and viewing sex work as ‘a choice’ and/or ‘deviant’ predicted greater victim blame, lower perceived criminality, and less anticipated harm. High self-reported empathy was the only predictor of greater anticipated harm. Results indicate the importance of understanding broader impacts of DSA - regardless of the victim - for stakeholders within the criminal justice system, and a continued need to generate public awareness of DSA through international policy.",2025-03-12T11:24:07.056983,,,
t7jfk_v1,Seeing is Believing: The Continued Influence of a Known AI-Generated 'Deepfake' Video,"Advances in artificial intelligence mean it is becoming easier to create highly realistic deepfake videos, which can appear to show someone doing or saying something they did not in fact do or say. Deepfakes may present a threat to individuals and society: for example, deepfakes could be used to influence elections by discrediting political opponents. Psychological research suggests that people cannot reliably identify deepfakes and thus can be influenced by their content. However, little is yet known about the potential impact of a deepfake video which has been explicitly identified and flagged as fake. We explored this issue with two preregistered studies in which participants were shown a bespoke deepfake video of a man appearing to admit to committing a crime. Participants were then asked questions about the man’s guilt, to test how they had been influenced by the video’s content. We found that most participants relied on the content of a deepfake video, even when they had been explicitly warned beforehand that it was fake. This result was observed even with participants who indicated that they believed the warning and knew the video to be fake. We also found this specific warning to be no more effective than a generic warning about the existence of deepfake videos. Our findings suggest that identifying and flagging deepfake videos will not entirely negate their influence. This has implications for legislators, policy makers, and regulators of social media platforms and online news.",2024-05-23T11:27:21.719145,,,
zhmy7_v1,Deepfake Detection in Super-Recognizers and Police Officers,"The present study is the first empirical investigation of the relationshion between human deepfake detection performance (DDP) and individuals' face identity processing ability. Using videos from the Deepfake Detection Challenge, we investigated DDP in two unique observer groups: Super-Recognizers (SRs) and ""normal"" officers from within the 18K members of the Berlin Police. SRs were identified either via previously proposed lab-based procedures or the only existing tool for SR identification involving increasingly challenging, authentic forensic material: the Berlin Test For Super-Recognizer Identification (beSure®). Participants judged either pairs of videos, or single videos in a 2-alternative forced-choice decision setting (i.e., which of the pair, or whether a single video was a deepfake or not). We explored speed-accuracy trade-offs, compared DDP between lab-identified SRs and non-SRs, and police officers as a function of their independently measured face identity processing (FIP) ability. Interestingly, we found no relationship between DDP and FIP ability. Further work using static deepfakes created with current state-of-the-art generative models is needed to determine the value of SR deployment for deepfake detection in law enforcement.",2024-01-13T20:21:39.496175,10.1109/MSEC.2024.3371030,,
a9dwe_v1,Cognitive flexibility but not cognitive styles influence deepfake detection skills and metacognitive accuracy,"Background: Deepfakes are AI-generated synthetic media that is increasingly used by cybercriminals to impersonate other individuals during remote social engineering attacks. Previous studies indicated that political orientation is associated with deepfake detection abilities, while being an IT professional is not. Little is known about the cognitive factors predicting individual differences in deepfake detection abilities. In this study, we assess the role of cognitive styles and cognitive flexibility on deepfake recognition skills and metacognitive accuracy. Methods: Cognitive styles and flexibility were measured using an embedded figures test that included a hidden cognitive flexibility task. 247 participants were tasked with rating a series of short video clips as either deepfake or authentic. Metacognitive accuracy was measured as prospective judgements of deepfake detection abilities controlling for actual performance. Results: Cognitive styles were not associated with deepfake detection performance. Cognitively flexible individuals were better at detecting deepfakes and had higher metacognitive accuracy than individuals who were less cognitively flexible. Conclusion: This is the first study assessing the role of cognitive styles and cognitive flexibility in deepfake detection skills and metacognitive judgements about deepfake detection abilities. Our results indicate that cognitively flexible individuals are better at detecting deepfakes and self-assessing social engineering susceptibility.",2023-02-14T15:03:01.665519,,,
whd4b_v1,The shallow of your smile: The ethics of expressive vocal deepfakes,"Rapid technological advances in artificial intelligence are creating opportunities for real-time algorithmic modulations of a person’s facial and vocal expressions, or “deep- fakes”. These developments raises unprecedented societal and ethical questions which, despite much recent public awareness, are still poorly understood from the point of view of moral psychology. We report here on an experimental ethics study conducted on a sample of N=303 participants (predominantly young, western and educated), who evaluated the acceptability of vignettes describing potential applications of expressive voice transformation technology. We found that vocal deep-fakes were generally well accepted in the population, notably in a therapeutic context and for emotions judged otherwise difficult to control, and surprisingly, even if the user lies to their interlocutors about using them. Unlike other emerging technologies like autonomous vehicles, there was no evidence of social dilemma in which one would e.g. accept for others what they resent for themselves. The only real obstacle to the massive deployment of vocal deep-fakes appears to be situations where they are applied to a speaker without their knowing, but even the acceptability of such situations was modulated by individual differences in moral values and attitude towards science-fiction.",2021-07-28T08:41:25.618196,,,
myu8h_v1,The shallow of your smile: The ethics of expressive vocal deepfakes,"Rapid technological advances in artificial intelligence are creating opportunities for real-time algorithmic modulations of a person’s facial and vocal expressions, or “deep-fakes”. These developments raises unprecedented societal and ethical questions which, despite much recent public awareness, are still poorly understood from the point of view of moral psychology. We report here on an empirical study conducted on N=303 online participants, who evaluated the acceptability of vignettes describing potential applications of expressive voice transformation technology. We found that vocal deep-fakes were generally well accepted in the population, notably in a therapeutic context and for emotions judged otherwise difficult to control, and surprisingly, even if the user lies to their interlocutors about using them. Unlike other emerging technologies like autonomous vehicles, there was no evidence of social dilemma in which one would e.g. accept for others what they resent for themselves. The only real obstacle to the massive deployment of vocal deep-fakes appears to be situations where they are applied to a speaker without their knowing, but even the acceptability of such situations was modulated by individual differences in moral values and attitude towards science-fiction.",2021-04-13T12:23:07.530986,,,
4ms5a_v1,Deepfaked online content is highly effective in manipulating people’s attitudes and intentions,"In recent times, disinformation has spread rapidly through social media and news sites, biasing our (moral) judgements of other people and groups. “Deepfakes”, a new type of AI-generated media, represent a powerful new tool for spreading disinformation online. Although Deepfaked images, videos, and audio may appear genuine, they are actually hyper-realistic fabrications that enable one to digitally control what another person says or does. Given the recent emergence of this technology, we set out to examine the psychological impact of Deepfaked online content on viewers. Across seven preregistered studies (N = 2558) we exposed participants to either genuine or Deepfaked content, and then measured its impact on their explicit (self-reported) and implicit (unintentional) attitudes as well as behavioral intentions. Results indicated that Deepfaked videos and audio have a strong psychological impact on the viewer, and are just as effective in biasing their attitudes and intentions as genuine content. Many people are unaware that Deepfaking is possible; find it difficult to detect when they are being exposed to it; and most importantly, neither awareness nor detection serves to protect people from its influence. All preregistrations, data and code available at osf.io/f6ajb.",2021-03-08T21:28:34.047805,,,
yq8f5_v1,"Celebrity status, sex, and variation in psychopathy predicts judgements of and proclivity to generate and distribute deepfake pornography","With the advent of means to generate and disseminate fake, sexualised images of others for the purposes of financial gain, harassment, or sexual gratification, there is a need to assess and understand the public’s awareness and judgements of said behaviour. In two independently-sampled studies, we used moderation (Study 1) and linear mixed effects (Study 2) analyses to investigate whether judgements of deepfaking differed as a function of victim status (celebrity, non-celebrity), victim and participant demographics, and image use (sharing, own sexual gratification), whilst controlling for the potential covariates of psychopathy and beliefs about a just world. We consistently observed more lenient judgements of deepfake generation and dissemination for victims who were celebrities and male, and when images were created for self-sexual gratification rather than being shared. Moreover, lenient judgements, as well as proclivity to act were predicted by greater levels of psychopathy. We discuss our findings in the context of needing to qualitatively understand the general public’s rationale for said disparity in judgements, as well as identifying and combating barriers to disclose victimisation. Open data and a preprint of this paper are available at https://osf.io/fp85q/?view_only=24ed2820782f4c2f9c19352f97d58611.",2020-11-15T17:11:36.789136,,,
