ID,Title,Abstract,Date Published,Tags,DOI,URL,Contributors,Provider
E0169-DC7-EB6,Perceptual judgements are resistant to the advisor's perceived level of trustworthiness: a deep fake approach,"As we navigate our environment, we often make spontaneous judgments about others. Trustworthiness is a key characteristic that we judge instantly and then use when making decisions, especially when we are uncertain. While the effect of trustworthiness on social behaviour has been extensively investigated, it remains unclear how trustworthiness affects more ‘basic’ cognitive processes such as perceptual decision-making. The present study aims to fill this gap. In the first experiment (N = 100), we validated a new trustworthiness manipulation by using deep fake technology to create animated versions of perceptually trustworthy, untrustworthy, and neutral static computer-generated faces. In the second experiment (N = 199), the deep fake procedure was applied to a new set of trustworthy and untrustworthy faces that served as advisors during a perceptual decision-making task. Here participants had to indicate the direction of dots that were either moving coherently to the left or to the right (i.e., random dot motion task). Contrary to our predictions, participants did not follow the advice of the trustworthy advisors more often than the advice of the untrustworthy advisors. We did find that participants decided faster and were more confident when following the advice, but this was also not influenced by the trustworthiness of the advisors. We integrate our findings within theoretical frameworks of advice taking, domain specificity of facial trustworthiness, and task requirements.",2024-05-09T12:35:09.876570+00:00,"deep fake,faces,social cognition,trustworthiness,advice taking,perceptual decision making",,,"Sam Verschooren, Marcel Brass, Mathias Van der Biest, Frederick Verbruggen",psyarxiv
E0101-0D4-1EE,Delineating non-consensual sexual image offending: Towards an empirical approach,"The topic of non-consensual sexual images has become an increasingly important issue within the social policy landscape. Social and legal scholars have advocated for these behaviours to be designated sexual offences due to the mode of perpetration of these behaviours, but are explicit in their rejection of a sexual element being important in the motivations underpinning such behaviours. However, this rejection is inconsistent with the core theoretical models related to sexual offending. In this article, we outline some of the potential psychological concepts that may help us to understand how and why people engage in a range of non-consensual sexual image offences, such as revenge pornography, upskirting, deepfake media production, and cyber-flashing. In doing so, we aim to begin to bridge the gap between legal scholars and psychological scientists, and develop a more comprehensive and theoretically coherent approach to studying this important social topic.",2019-06-12T19:58:21.005041+00:00,"upskirting,courtship disorder,cyber-flashing,non-consensual sexual images,dick pics,sexual offending,deepfake,revenge pornography,exhibitionism,pornography",,,"Craig A. Harper, Dean Fido, Dominic Petronzi",psyarxiv
E00C9-744-B93,Deepfaked online content is highly effective in manipulating people’s attitudes and intentions,"In recent times, disinformation has spread rapidly through social media and news sites, biasing our (moral) judgements of other people and groups. “Deepfakes”, a new type of AI-generated media, represent a powerful new tool for spreading disinformation online. Although Deepfaked images, videos, and audio may appear genuine, they are actually hyper-realistic fabrications that enable one to digitally control what another person says or does. Given the recent emergence of this technology, we set out to examine the psychological impact of Deepfaked online content on viewers. Across seven preregistered studies (N = 2558) we exposed participants to either genuine or Deepfaked content, and then measured its impact on their explicit (self-reported) and implicit (unintentional) attitudes as well as behavioral intentions. Results indicated that Deepfaked videos and audio have a strong psychological impact on the viewer, and are just as effective in biasing their attitudes and intentions as genuine content. Many people are unaware that Deepfaking is possible; find it difficult to detect when they are being exposed to it; and most importantly, neither awareness nor detection serves to protect people from its influence. All preregistrations, data and code available at osf.io/f6ajb.",2021-03-08T21:28:34.047805+00:00,"implicit,disinformation,psychology,deepfake,intentions,attitudes",,,"Rian Hughes, Ciaran Hughes, Sean Hughes, Xinwei Yao, Melissa Ferguson, Ian Hussey, Ohad Fried",psyarxiv
E0071-DDA-89C,Seeing is Believing: The Continued Influence of a Known AI-Generated 'Deepfake' Video,"Advances in artificial intelligence mean it is becoming easier to create highly realistic deepfake videos, which can appear to show someone doing or saying something they did not in fact do or say. Deepfakes may present a threat to individuals and society: for example, deepfakes could be used to influence elections by discrediting political opponents. Psychological research suggests that people cannot reliably identify deepfakes and thus can be influenced by their content. However, little is yet known about the potential impact of a deepfake video which has been explicitly identified and flagged as fake. We explored this issue with two preregistered studies in which participants were shown a bespoke deepfake video of a man appearing to admit to committing a crime. Participants were then asked questions about the man’s guilt, to test how they had been influenced by the video’s content. We found that most participants relied on the content of a deepfake video, even when they had been explicitly warned beforehand that it was fake. This result was observed even with participants who indicated that they believed the warning and knew the video to be fake. We also found this specific warning to be no more effective than a generic warning about the existence of deepfake videos. Our findings suggest that identifying and flagging deepfake videos will not entirely negate their influence. This has implications for legislators, policy makers, and regulators of social media platforms and online news.",2024-05-23T11:27:21.719145+00:00,"artificial intelligence,deepfake,misinformation,social media,psychology",,,"Simon Clark, Stephan Lewandowsky",psyarxiv
E005F-432-924,"How deepfake quality, media literacy, and personal attitudes shape detection, liking, and social media sharing of political deepfakes","The increasing realism and accessibility of political deepfakes pose serious risks to democratic discourse by blurring the line between authentic and manipulated media. The present study examined how deepfake quality, media literacy, and attitudes toward the person depicted in the video shape individuals’ ability to detect political deepfakes, their emotional responses (liking), and behavioral intentions (social media sharing). A total of 1,124 participants from the United Kingdom, Slovenia, and Italy viewed manipulated videos about climate change and immigration that varied in quality. The results showed that high-quality deepfakes were less likely to be detected and received more positive evaluations than low-quality ones. In contrast, quality did not significantly influence sharing intention. Media literacy and attitudes toward the person in the video emerged as strong predictors across outcomes; higher media literacy improved detection and reduced liking and sharing, while more positive attitudes decreased detection and increased both liking and sharing. Mediation analyses demonstrated that liking partially mediated the link between detection and sharing intention. Furthermore, moderated mediation models revealed that this indirect effect was stronger among individuals with high media literacy (for climate change videos) and favorable attitudes toward the video’s subject (for both topics). Overall, the study highlights the need for interventions that address not only detection skills but also emotional and motivational susceptibility to persuasive synthetic media. Enhancing media literacy and implementing platform-level friction mechanisms may help curb the spread of political deepfakes online.",2025-05-19,"attitudes,deepfake,detection,liking,media literacy,quality,sharing",,,"Urška Smrke, Michele Brienza, Nejc Plohl, Izidor Mlakar, Letizia Aquilino, Piercosma Bisconti",psyarxiv
E0190-06C-F6A,Deepfake Detection in Super-Recognizers and Police Officers,"The present study is the first empirical investigation of the relationshion between human deepfake detection performance (DDP) and individuals' face identity processing ability. Using videos from the Deepfake Detection Challenge, we investigated DDP in two unique observer groups: Super-Recognizers (SRs) and ""normal"" officers from within the 18K members of the Berlin Police. SRs were identified either via previously proposed lab-based procedures or the only existing tool for SR identification involving increasingly challenging, authentic forensic material: the Berlin Test For Super-Recognizer Identification (beSure®). Participants judged either pairs of videos, or single videos in a 2-alternative forced-choice decision setting (i.e., which of the pair, or whether a single video was a deepfake or not). We explored speed-accuracy trade-offs, compared DDP between lab-identified SRs and non-SRs, and police officers as a function of their independently measured face identity processing (FIP) ability. Interestingly, we found no relationship between DDP and FIP ability. Further work using static deepfakes created with current state-of-the-art generative models is needed to determine the value of SR deployment for deepfake detection in law enforcement.",2024-01-13T20:21:39.496175+00:00,"law enforcement,policing,super-recognizers,deepfake detection,facial identity processing",,,"Matthew J Vowels, Matthew Groh, Meike Ramon",psyarxiv
E001C-1FF-3E1,Labeling Synthetic Content: User Perceptions of Warning Label Designs for AI-generated Content on Social Media,"In this research, we explored the efficacy of various warning label designs for AI-generated content on social media platforms—e.g., deepfakes. We devised and assessed ten distinct label design samples that varied across the dimensions of sentiment, color/iconography, positioning, and level of detail. Our experimental study involved 911 participants randomly assigned to these ten label designs and a control group evaluating social media content. We explored their perceptions relating to 1) Belief in the content being AI-generated, 2) Trust in the labels and 3) Social Media engagement perceptions of the content. The results demonstrate that the presence of labels had a significant effect on the user’s belief that the content is AI-generated, deepfake, or edited by AI. However their trust in the label significantly varied based on the label design. Notably, having labels did not significantly change their engagement behaviors, such as ’like’, comment, and sharing. However, there were significant differences in engagement based on content type: political and entertainment. This investigation contributes to the field of human-computer interaction by defining a design space for label implementation and providing empirical support for the strategic use of labels to mitigate the risks associated with synthetically generated media.",2025-02-20T11:57:42.989521+00:00,"warning labels,content labels,#ai content,deepfake images,deepfakes",,,"Arosha Bandara, Dilki, Dilrukshi Gamage, Min Zhang",psyarxiv
E01F5-022-85A,"Celebrity status, sex, and variation in psychopathy predicts judgements of and proclivity to generate and distribute deepfake pornography","With the advent of means to generate and disseminate fake, sexualised images of others for the purposes of financial gain, harassment, or sexual gratification, there is a need to assess and understand the public’s awareness and judgements of said behaviour. In two independently-sampled studies, we used moderation (Study 1) and linear mixed effects (Study 2) analyses to investigate whether judgements of deepfaking differed as a function of victim status (celebrity, non-celebrity), victim and participant demographics, and image use (sharing, own sexual gratification), whilst controlling for the potential covariates of psychopathy and beliefs about a just world. We consistently observed more lenient judgements of deepfake generation and dissemination for victims who were celebrities and male, and when images were created for self-sexual gratification rather than being shared. Moreover, lenient judgements, as well as proclivity to act were predicted by greater levels of psychopathy. We discuss our findings in the context of needing to qualitatively understand the general public’s rationale for said disparity in judgements, as well as identifying and combating barriers to disclose victimisation. Open data and a preprint of this paper are available at https://osf.io/fp85q/?view_only=24ed2820782f4c2f9c19352f97d58611.",2020-11-15T17:11:36.789136+00:00,"non-consensual image-based offending,judgements,psychopathy,deepfake media production",,,"Dean Fido, Craig A. Harper, Jaya Rao",psyarxiv
E0153-ACC-2E6,Are we overclaiming the Psychosocial Impacts of Misinformation? A Systematic Review of Evidence,"Misinformation, or what we call ‘Technologically manipulated information’ (TMI), has a long history of shaping people's social lives. Research in this area has spiked recently, especially after the COVID-19 pandemic. With technological advancement, the ease of manipulating information, as in ‘deep fake,’ and its quick dissemination over social media has caused a stir among researchers. ’ However, in the relevant literature, a general assumption is repeatedly observed that misinformation and TMI lead to negative psychosocial impacts. The present review systematically analyzes the empirical evidence available in the research literature for the negative impact of misinformation. The aim is to gather proof for this commonly held belief about the impact and identify the gaps in the literature, if any.",2023-07-06T03:45:14.343020+00:00,"misinformation,ai,social networking sites,covid 19,psychosocial impact,and deepfakes",,,Ankita Sharma,psyarxiv
E01DD-770-81D,Enhancing Human Detection of Real and AI-Generated Hyperrealistic Faces,"Recent advancements in generative artificial intelligence (AI) technologies have prompted concerns about the proliferation of deepfake content—synthetic images, audio, and videos—online. StyleGan2, a powerful generative adversarial network architecture, can generate synthetic images of human faces that appear more realistic than actual human faces—a phenomenon termed AI hyperrealism. Across four preregistered experiments (total N = 661), we tested whether a novel behavioral intervention, termed DISCERN-AI, improved people’s ability to discriminate between real and synthetic StyleGan2 images. DISCERN-AI combines instructions about which visual features are diagnostic for classifying real and synthetic images with an inductive learning training protocol. In Experiment 1, participants completed a pre-test, DISCERN-AI, and a post-test. In Experiment 2, participants underwent DISCERN-AI or a control task before completing a post-test. In Experiments 3A and 3B, participants completed a shortened version of the intervention or control task, followed by an immediate post-test and a second post-test after 20 days. In all experiments, participants showed below-chance discrimination performance (AI hyperrealism) in the pre-test and control conditions. After completing DISCERN-AI, however, participants consistently showed above-chance discrimination performance, even when tested after 20 days. Together, the results provide support for an easily scalable intervention that substantially improves people’s ability to discriminate between real and synthetic StyleGan2 images.",2025-02-27T19:03:39.270656+00:00,"face perception,deepfake detection,ai hyperrealism,generative ai,artificial intelligence (ai),roc analysis",,,"Philip Anthony Higham, Mansi Pattni, Tina Seabrooke",psyarxiv
E003B-D9B-A86,Understanding Image-Based Sexual Abuse through Greek Public and Legal Lenses: A Constructionist Thematic Analysis,"Image-Based Sexual Abuse (IBSA), including deepfake sexual abuse - where fake yet lifelike sexual content is generated of non-consenting persons - constitutes a growing form of digitally mediated gender-based violence that remains largely under-researched within non-Anglophonic contexts. This study explores how IBSA is constructed and perceived by both laypeople and lawyers in Greece, a Southern European setting characterised by economic precarity, traditional gender norms, and evolving yet challenging legal frameworks. Drawing on semi-structured interviews with 21 participants either originating from or living in Greece (n = 16 lay persons, n = 5 legal professionals), we employed constructionist thematic analysis to examine how participants discursively frame IBSA, its motivations, barriers to reporting, legal challenges, and preventative measures. Five themes were identified: (1) constructions of IBSA as gendered violence motivated by control, humiliation, and financial exploitation; (2) barriers to reporting shaped by shame, stigma, and widespread mistrust in police institutions; (3) legal barriers related to financial inaccessibility and fragmented, outdated legal frameworks; (4) the central role of informal support networks alongside calls for education and public awareness; and (5) deepfake technologies as an emergent form of economic exploitation, particularly impacting sex workers within a legal vacuum. The findings highlight the need for structurally informed, context-sensitive responses to IBSA that address the intersections of gender, law, technology, and economic vulnerability.",2025-07-03,,,,"Anastasia Rousaki, Dean Fido",psyarxiv
E01D3-B7B-DB4,"Individual Deep Fake Recognition Skills are Affected by Viewers’ Political Orientation, Agreement with Content and Device Used","AI-generated “deep fakes” are becoming increasingly professional and can be expected to become an essential tool for cybercriminals conducting targeted and tailored social engineering attacks, as well as for others aiming for influencing public opinion in a more general sense. While the technological arms race is resulting in increasingly efficient forensic detection tools, these are unlikely to be in place and applied by common users on an everyday basis any time soon, especially if social engineering attacks are camouflaged as unsuspicious conversations. To date, most cybercriminals do not yet have the necessary resources, competencies or the required raw material featuring the target to produce perfect impersonifications. To raise awareness and efficiently train individuals in recognizing the most widespread deep fakes, the understanding of what may cause individual differences in the ability to recognize them can be central. Previous research suggested a close relationship between political attitudes and top-down perceptual and subsequent cognitive processing styles. In this study, we aimed to investigate the impact of political attitudes and agreement with the political message content on the individual’s deep fake recognition skills. In this study, 163 adults (72 females = 44.2%) judged a series of video clips with politicians’ statements across the political spectrum regarding their authenticity and their agreement with the message that was transported. Half of the presented videos were fabricated via lip-sync technology. In addition to the particular agreement to each statement made, more global political attitudes towards social and economic topics were assessed via the Social and Economic Conservatism Scale (SECS). Data analysis revealed robust negative associations between participants’ general and in particular social conservatism and their ability to recognize fabricated videos. This effect was pronounced where there was a specific agreement with the message content. Deep fakes watched on mobile phones and tablets were considerably less likely to be recognized as such compared to when watched on stationary computers. To the best of our knowledge, this study is the first to investigate and establish the association between political attitudes and interindividual differences in deep fake recognition. The study further supports very recently published research suggesting relationships between conservatism and perceived credibility of conspiracy theories and fake news in general. Implications for further research on psychological mechanisms underlying this effect are discussed.",2021-12-30T14:24:35.177365+00:00,"fake news,deepfakes,conservatism,deep fakes,recognition,political orientation,cybercrime,perception,social media,social engineering",,,"Matthew Canham, Leandra Wolf, Torvald F. Ask, Teodora Bursac, Stefan Sütterlin, Julian Schray, Sandra Glöckler, Ali Khodabakhsh, Alaya Chandi, Sophia Mägerle, Benjamin J. Knox, Ricardo Lugo",psyarxiv
E00BD-0A7-E14,Judgements of Deepfake Sexual Abuse Victims Differ as a Function of Facial Versus Body Likenesses,"We are witnessing exponential growth in the use of machine learning to create fake – yet indistinguishable - sexual media of others without their consent. Though there is an emerging understanding of the impact that deepfake sexual abuse (DSA) has on its victims and societal understanding thereof, this knowledge pertains entirely to individuals whose facial likeness has been used within said material, with little-to-no attention paid to those whose bodies are used as the canvas; individuals who are predominantly sex workers. Across 321 participants (Mage = 45.70 years, SD = 15.88; 48.9% female), vignettes were used to explore differences in societal judgements of DSA for victims whose face (versus body) was used to generate DSA material, and whether they were labelled as a sex worker. Though perceived criminality did not differ across conditions, participants allocated more blame and less anticipated harm to DSA victims whose body, relative to whose face, was used. This effect was enhanced in vignettes labelling them as sex workers. When exploring correlations using demographics, beliefs, and personality traits, being older, male, and viewing sex work as ‘a choice’ and/or ‘deviant’ predicted greater victim blame, lower perceived criminality, and less anticipated harm. High self-reported empathy was the only predictor of greater anticipated harm. Results indicate the importance of understanding broader impacts of DSA - regardless of the victim - for stakeholders within the criminal justice system, and a continued need to generate public awareness of DSA through international policy.",2025-03-12,"OnlyFans,deepfake sexual abuse,image-based sexual abuse,sex workers,technology-facilitated sexual violence",,,"Dominic Ruddy, Dean Fido, Harriet Goldfinch, Craig Harper",psyarxiv
E006A-869-84B,Trading faces: Complete AI face doubles avoid the uncanny valley,"Advances in artificial intelligence (AI) enable the creation of videos in which a person appears to say or do things they did not. The impact of these so-called “deepfakes” hinges on their perceived realness. Here we tested different versions of deepfake faces for Welcome to Chechnya, a documentary that used face swaps to protect the privacy of Chechen torture survivors who were persecuted because of their sexual orientation. AI face swaps that replace an entire face with another were perceived as more human-like and less unsettling compared to partial face swaps that left the survivors’ original eyes unaltered. The full-face swap was deemed the least unsettling even in comparison to the original (unaltered) face. When rendered in full, AI face swaps can appear human and avoid aversive responses in the viewer associated with the uncanny valley.",2020-07-08T20:45:26.536497+00:00,"film,uncanny valley,face swap,welcome to chechnya,face perception",,,"Thalia Wheatley, Christopher Welker, Alice Henty, David France",psyarxiv
E0030-8D9-D34,Seeing is Deceiving: The Psychology and Neuroscience of Fake Faces,"Face perception is an essential skill for human interaction and social learning, impacting how we build relationships and perceive the world around us. In the modern era, face processing remains important but confronts new challenges due to recent technological advances such as deepfake technology and AI-generated faces. These computer-generated (CG) faces may be difficult for our brains to distinguish from real faces, raising questions in the fields of criminal justice, politics, and animation, to name a few. This review explores the neurobiology of face processing and its interplay with affect, laying the foundation for an investigation into recent studies examining how humans differentiate between real and CG faces. Studies on the uncanny valley effect and pareidolia offer further insights as to how humans make this judgment and the possible boundaries of face perception. Additional research is needed to better understand this emerging area and possibly train human viewers to perform these judgments more accurately in the future.",2020-05-27T22:12:45.326933+00:00,"face judgments,face processing,computer-generated faces,pareidolia,uncanny valley",,,Sujal Manohar,psyarxiv
E0035-40C-80C,What label should be applied to content produced by generative AI?,"The rise of generative AI has created pressure for content labeling. This paper investigates the public’s understanding of nine potential labels. Participants from the US (N=1056), Mexico (N=1060), Brazil (N=1065), India (N=1038), and China (N=1031) were shown twenty different types of content that varied in the extent to which they were AI-generated, and the extent to which they were misleading. Across countries and demographic subgroups, participants consistently associated “AI Generated,” “Generated with an AI tool,” and “AI manipulated” with AI-generated content, regardless of misleadingness; and associated “Deepfake” and “Manipulated” with mis- leading content, regardless of AI involvement. Interestingly, “Artificial” performed poorly in China due to translation nuances, but performed well on both alignment tasks in the other countries. Finally, we examined self-reported effects of the terms on belief in, and attitudes toward, labeled content. Our study underscores the need for deliberate decision-making regarding the objectives and implementation of generative AI disclosure.",2023-07-28T18:30:34.007114+00:00,"labeling,artificial intelligence,misinformation,social media,generative ai",,,"Ziv Epstein, Mengying Cathy Fang, Antonio Alonso Arechar, David Rand",psyarxiv
E00EC-6A3-D04,Cloned voices are easier to understand in noise than their human originals: the voice cloning intelligibility benefit,"Voice cloning technology has developed at an extremely rapid pace and recent work suggests the quality of synthesis now produces humanlike voices. Recent research on cloned voices focused on listeners’ capabilities to discriminate between cloned, ‘deepfake’ and human voices, due to perils associated with the misuse of this technology. However, the relative intelligibility of cloned and human voices is unknown. We compared the relative intelligibility ten human voices with their ten clones in background noise. Participants (N=80) listened to 80 sentences, 40 human and 40 cloned in +3dB, 0dB, -3dB and -6dB signal-to-noise ratios (SNR) in an online experiment. We found that cloned voices were more intelligible in noise than their human counterparts; up to 20% higher across all four noise levels. We also asked participants to rate the clarity and accent strength of all human and cloned voices; and asked participants to identify which voice was human in an 2AFC task. The cloned voices were rated as having marginally higher clarity and perceived to have a less standard accent. Participants identified the human voices with ~70% accuracy. The acoustic analysis of both types of voices revealed that the intelligibility benefit was linked to voice source characteristics including mean pitch and period variation, plus a smoother voice source and improved harmonic-to-noise ratio in the 500-3500 frequency range in the cloned voices. Our results have implications for applications of cloned voices, such as voice restoration, speech synthesis for non-verbal people, and for people with hearing loss.",2025-08-16,,,,"patti adank, Han Wang",psyarxiv
